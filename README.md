# Verbose ListOps Evaluation Benchmark

**EXPERIMENTAL — WORK IN PROGRESS**

## Overview

Welcome to the Verbose ListOps Evaluation Benchmark repository. This project provides tools to generate challenging evaluation tasks for sequence models, particularly Large Language Models (LLMs). The benchmark assesses reasoning capabilities when faced with tasks embedded within extremely long, narratively structured, and potentially distracting contexts.

It builds upon the 2018 [ListOps benchmark](https://arxiv.org/abs/1804.06028) by transforming its concise, symbolic problem representation into lengthy, natural language narratives generated dynamically using an LLM (currently Anthropic's Claude). The core Python script, `verbose-listops.py`, orchestrates this process.

Whilst the underlying computational task remains identical to the original ListOps benchmark, even SOTA models will find processing the narratives generated by verbose-listops extremely challenging.

Standard long-context tests often focus on "needle-in-a-haystack" retrieval – finding a specific fact within a large document. Verbose ListOps is significantly more challenging, requiring not just finding information, but understanding relationships, extracting structured operations, ignoring sophisticated distractions, and performing multi-step computations based on information scattered across potentially enormous token lengths.

Due the nature of this computational task, this dataset is especially well-suited to understanding the ability of a large language model to extract and score arbitrarily defined predictive signals from a large corpus of unstructured text; for example, extracting and scoring qualification signals from sales transcripts.

## Comparison with Similar Benchmarks

While Verbose ListOps has a unique combination of features (ListOps core task, LLM-generated narrative context, specific padding mechanism), several other benchmarks evaluate related aspects of long-context reasoning. Here's a comparison:

| Feature | Verbose ListOps | BABILong | LongReason |
| :------ | :-------------- | :------- | :--------- |
| **Core Reasoning Task** | ListOps (hierarchical arithmetic & list operations: MAX, MIN, MED, SUM, SM, AVG) | 20 types derived from bAbI tasks (fact chaining, simple induction, deduction, counting, lists/sets, etc.) | Diverse existing short reasoning questions (reading comprehension, logical inference, math word problems) |
| **Context Generation** | LLM generates a narrative embedding ListOps steps, includes world-building & specific "padding" generation. | Embeds bAbI task facts within long background text from existing corpora (e.g., PG19 books, Wikipedia). | Synthetically expands the context around existing short reasoning questions.                                         |
| **Distraction Type** | Coherent, LLM-generated narrative, characters, plot elements, and *contextually relevant* padding. | Natural language text (potentially unrelated) from large documents surrounding key facts. | Depends on expansion method; likely added text that might be related or unrelated, potentially less narrative. |
| **Context Length** | Configurable, designed for >50k tokens. | Highly scalable; datasets generated up to 1M or 10M tokens claimed. | Tested up to 128k tokens in the original paper. |
| **Primary Goal** | Test *highly nested* reasoning & extraction within very long, LLM-generated *narrative* context, focusing on resistance to plausible distraction. | Test reasoning across facts sparsely distributed within extremely long documents ("reasoning-in-a-haystack"). | Evaluate performance degradation on known reasoning tasks as context length increases via expansion. |
| **Strengths** | - Tests resistance to highly relevant, narrative distraction.<br>- LLM-generated context potentially mimics future noisy data.<br>- Core task requires precise hierarchical execution.<br>- Highly configurable generation process. | - Uses a diverse set of established basic reasoning tasks (bAbI).<br>- Employs natural text for background context.<br>- Very high scalability in context length demonstrated.<br>- Established benchmark with published results. | - Leverages well-understood existing reasoning questions.<br>- Covers multiple types of reasoning.<br>- Directly measures impact of context length on known task performance. |
| **Weaknesses** | - Narrative quality/distraction level can be variable.<br> | - Core bAbI task is synthetically simplistic.<br>- Background text distraction is less targeted.<br>- Performance heavily relies on finding sparse facts in potentially unrelated text. | - "Context expansion" method is artificial and less realistic.<br>- Effectiveness depends heavily on *how* context is added.<br>- May not fully capture challenges of naturally long/narrative texts. |

## The Core ListOps Task

The underlying computational task remains identical to the original ListOps benchmark. Models processing the generated narrative must effectively:

1.  Identify and parse nested operations on lists embedded within the story.
2.  The supported operations are:
    * `MAX` (maximum)
    * `MIN` (minimum)
    * `MED` (median)
    * `SUM` (sum)
    * `SM` (sum modulo 10)
    * `AVG` (integer average, floored)
3.  Operands are single-digit integers (0-9), also described textually within the narrative.
4.  Evaluate these nested operations hierarchically according to the narrative structure.
5.  Produce a single-digit integer as the final result based on the root operation.

ListOps fundamentally tests a model's ability to understand hierarchical structures and perform sequential reasoning.

## Design Goal: Testing Large Context Windows with Narrative Complexity

The primary motivation remains to rigorously evaluate model performance on tasks involving extremely large context windows. However, this implementation focuses on achieving verbosity and complexity through **LLM-driven narrative generation** rather than just injecting unrelated filler text.

Standard benchmarks may not sufficiently stress a model's ability to:

* Maintain coherence and track dependencies over tens of thousands of tokens within a continuous story.
* Extract relevant operational information (operators, operands) from narrative descriptions interwoven with characters, setting, and plot elements.
* Perform multi-step reasoning when instructions are presented as part of an unfolding story across a vast input sequence.
* Resist distraction from contextually relevant but computationally irrelevant "padding" generated by the LLM (e.g., side-quests, character interactions).

This benchmark aims to address this by programmatically generating long narratives where the ListOps task is the hidden "logic puzzle" the model must solve. Target context lengths can reach configurable limits (e.g., 10,000 tokens or more, adjustable via parameters).

## Methodology: From ListOps AST to Generated Narrative

The `verbose-listops.py` script implements the following process:

1.  **Random AST Generation:** A random ListOps Abstract Syntax Tree (AST) is constructed (`build_random_ast`) defining the nested structure and operations of the problem. Parameters control the maximum depth and branching factor. The AST is evaluated (`eval_node`) to determine the ground truth answer.
2.  **World Building:** An initial call is made to the Anthropic API (`generate_world`) to create a fictional context, including characters, genre, and setting. This provides a consistent backdrop for the narrative.
3.  **Narrative Generation (`generate_narrative`):**
    * The script traverses the AST (pre-order).
    * For each **operator node** in the AST, it prompts the LLM (Anthropic API via `_client_create`) to write a short "story beat" or scene. This prompt includes:
        * The generated world context (characters, setting, genre).
        * The specific ListOps operation (`op`), its operands (`operands`), and the intermediate result (`value`) for that node.
        * Instructions to weave this logic into the narrative involving the characters, *without explicitly revealing the numeric answer as a meta-clue*.
    * **Narrative Padding:** After generating a story beat for an operation, the script may make additional calls to the LLM to generate "padding" paragraphs. This padding is prompted to continue the *last scene* with side-quests, mysteries, or random asides, explicitly instructed *not* to change established facts or operator logic. This increases verbosity using contextually relevant, LLM-generated text.
    * **Token Management:** The generation process respects configurable token limits for individual beats (`MAX_BEAT_TOKENS`), padding sections (`MAX_PAD_TOKENS`), and the overall narrative (`MAX_TOTAL_TOKENS`), ensuring the output stays within desired bounds.
4.  **Question Generation:** After generating the full narrative based on the AST, a final prompt is sent to the LLM to formulate a question specifically asking for the result of the *top-level* operation performed in the story, using few-shot examples for guidance.

The result is a single block of text containing the full narrative followed by the target question.

## Why Verbose ListOps (LLM-Generated Version)?

This approach provides a challenging test case where the core task is simple, but requires the evaluated model to:

* Read and understand a long, coherent narrative generated by another LLM.
* Identify and isolate the embedded computational steps from the story elements.
* Track the hierarchy and dependencies of the operations as presented narratively.
* Answer a direct question about the final result of the embedded ListOps task.

It tests whether models can truly *use* their long context for complex reasoning within noisy, narratively structured data, going beyond simple filler injection.

## Code Implementation Details

The `verbose-listops.py` script includes:

* **Dependencies:** `anthropic` (for API calls), `tiktoken` (for token counting).
* **Core Logic:** AST generation, evaluation, world generation, narrative beat/padding generation via LLM calls, question generation.
* **Configuration:** Constants at the top of the script control:
    * API Model (`MODEL`)
    * Token limits (`MAX_TOTAL_TOKENS`, `MAX_BEAT_TOKENS`, `MAX_PAD_TOKENS`)
    * AST structure (`DEFAULT_MAX_BRANCH`, `MIN_ARITY`, `ATOM_MIN_VALUE`, `ATOM_MAX_VALUE`)
    * Retry behavior (`RETRY_MAX_ATTEMPTS`, `RETRY_INITIAL_DELAY`)
    * Logging (`LOG_DIR`, `LOG_MAX_BYTES`, `LOG_BACKUP_COUNT`)
* **API Key:** Requires the `ANTHROPIC_API_KEY` environment variable to be set.
* **Error Handling:** Includes retry logic (`retry_api_call`) for API calls with exponential backoff.
* **Logging:** Detailed logs are written to `~/verbose_listops_logs/verbose_listops.log`, including prompts sent to the LLM. World metadata is saved to `world.json` in the same directory.

## Usage

1.  **Setup:**
    * Ensure you have Python installed.
    * Install required libraries: `pip install anthropic tiktoken`
    * Set the `ANTHROPIC_API_KEY` environment variable: `export ANTHROPIC_API_KEY='your-api-key'`
2.  **Run the Script:** Execute `python verbose-listops.py`.
3.  **Output:**
    * The script will print the final generated narrative followed by the question to the standard output.
    * Detailed logs and the generated `world.json` will be saved in `~/verbose_listops_logs/`.
4.  **Evaluation:**
    * Provide the full narrative text (including the final question) as input to the LLM you want to evaluate.
    * Parse the model's response to extract the predicted single-digit answer.
    * Compare the prediction against the ground truth answer (which can be found by running the script or inspecting the logs/AST evaluation logic if needed, though ideally the generation script is trusted). Calculate accuracy or other relevant metrics.

## Example Conceptual Flow

Consider the standard ListOps problem: `[SM 8 1 4 [MAX 9 2 7]]` (Answer: 2)

The script would execute steps like this:

1.  **AST:** Generate an AST representing this structure. Evaluate it to confirm the answer is 2.
2.  **World:** Call the LLM to create a world (e.g., Genre: Sci-Fi, Setting: Space Station, Characters: Captain Eva Rostova, Engineer Jax, Alien Diplomat Zorp).
3.  **Narrative (Simplified):**
    * **Beat 1 (MAX):** Prompt LLM: "Write a scene where Eva, Jax, and Zorp encounter numerical readings 9, 2, 7, and focus on the *largest* one (9) as being critical." -> LLM generates a paragraph or two about finding the critical reading 9.
    * *Padding:* Prompt LLM: "Continue the scene, maybe Jax has a side-task." -> LLM adds paragraphs about Jax fixing a panel.
    * **Beat 2 (SM):** Prompt LLM: "Write a scene where the primary task involves combining readings 8, 1, 4, and the previous critical result (9), then finding the *sum modulo 10* (resulting in 2) to unlock a door." -> LLM generates paragraphs about entering the sequence and the door unlocking with code 2.
    * *Padding:* Prompt LLM: "Continue the scene, maybe Zorp comments on human inefficiency." -> LLM adds Zorp's dialogue.
4.  **Question:** Prompt LLM: "Based on the story ending with the sum modulo 10 operation, formulate a question asking for that final result." -> LLM generates: "Following the procedures described in the narrative, what was the final code generated by the sum modulo 10 operation at the main level?"

*(Note: The actual generated text will be far more verbose and variable due to the LLM's creative generation.)*